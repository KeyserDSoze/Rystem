# Multi-Modal Support Implementation Plan

## üéØ Obiettivo

Aggiungere supporto multi-modale a Rystem.PlayFramework per consentire:
- ‚úÖ **Immagini** (input e output)
- ‚úÖ **Audio** (input e output) 
- ‚úÖ **File/Documenti** (PDF, DOCX, ecc.)
- ‚úÖ **Video** (input)

## üìä Architettura Attuale (Text-Only)

```csharp
// Attuale: Solo testo
await sceneManager.ExecuteAsync(
    message: "Analizza questa immagine",
    metadata,
    settings
);
```

## üé® Architettura Proposta (Multi-Modal)

### 1. Nuovo Modello: `MultiModalInput`

```csharp
public sealed class MultiModalInput
{
    /// <summary>
    /// Text message (may be null for image-only requests)
    /// </summary>
    public string? Text { get; init; }

    /// <summary>
    /// Multi-modal contents (images, audio, files)
    /// </summary>
    public List<AIContent> Contents { get; init; } = [];

    /// <summary>
    /// Helper: Create from text only
    /// </summary>
    public static MultiModalInput FromText(string text) => new() { Text = text };

    /// <summary>
    /// Helper: Create from image URL
    /// </summary>
    public static MultiModalInput FromImageUrl(string text, string imageUrl) => new()
    {
        Text = text,
        Contents = [new ImageContent(imageUrl)]
    };

    /// <summary>
    /// Helper: Create from image bytes (base64)
    /// </summary>
    public static MultiModalInput FromImageBytes(string text, byte[] imageBytes, string mimeType = "image/png") => new()
    {
        Text = text,
        Contents = [new ImageContent(Convert.ToBase64String(imageBytes), mimeType)]
    };

    /// <summary>
    /// Helper: Create from audio URL
    /// </summary>
    public static MultiModalInput FromAudioUrl(string text, string audioUrl) => new()
    {
        Text = text,
        Contents = [new AudioContent(audioUrl)]
    };

    /// <summary>
    /// Helper: Create from file (PDF, DOCX, etc.)
    /// </summary>
    public static MultiModalInput FromFileUrl(string text, string fileUrl) => new()
    {
        Text = text,
        Contents = [new DataContent(fileUrl)]
    };

    /// <summary>
    /// Convert to ChatMessage with text + contents
    /// </summary>
    public ChatMessage ToChatMessage(ChatRole role = ChatRole.User)
    {
        var contents = new List<AIContent>();

        // Add text if present
        if (!string.IsNullOrWhiteSpace(Text))
        {
            contents.Add(new TextContent(Text));
        }

        // Add multi-modal contents
        contents.AddRange(Contents);

        return new ChatMessage(role, contents);
    }
}
```

### 2. Modifica `ISceneManager`

```csharp
public interface ISceneManager
{
    /// <summary>
    /// Executes a scene request with multi-modal input support.
    /// </summary>
    /// <param name="input">Multi-modal input (text, images, audio, files)</param>
    /// <param name="metadata">Request metadata</param>
    /// <param name="settings">Request settings</param>
    /// <param name="cancellationToken">Cancellation token</param>
    /// <returns>Stream of responses</returns>
    IAsyncEnumerable<AiSceneResponse> ExecuteAsync(
        MultiModalInput input,
        Dictionary<string, object>? metadata = null,
        SceneRequestSettings? settings = null,
        CancellationToken cancellationToken = default);

    /// <summary>
    /// Executes a scene request with text-only input (backward compatibility).
    /// </summary>
    IAsyncEnumerable<AiSceneResponse> ExecuteAsync(
        string message,
        Dictionary<string, object>? metadata = null,
        SceneRequestSettings? settings = null,
        CancellationToken cancellationToken = default)
    {
        return ExecuteAsync(MultiModalInput.FromText(message), metadata, settings, cancellationToken);
    }
}
```

### 3. Modifica `SceneContext`

```csharp
public sealed class SceneContext
{
    /// <summary>
    /// User's input (multi-modal support)
    /// </summary>
    public required MultiModalInput Input { get; set; }

    /// <summary>
    /// User's input message (text part, for backward compatibility)
    /// </summary>
    [Obsolete("Use Input property instead for multi-modal support")]
    public string InputMessage 
    { 
        get => Input.Text ?? string.Empty; 
        set => Input = MultiModalInput.FromText(value); 
    }

    // ... rest of properties
}
```

### 4. Modifica `AiSceneResponse` (Optional - per output multi-modale)

```csharp
public sealed class AiSceneResponse
{
    // ... existing properties

    /// <summary>
    /// Multi-modal output contents (images, audio generated by LLM)
    /// </summary>
    public List<AIContent>? Contents { get; set; }

    /// <summary>
    /// Helper: Check if response contains image
    /// </summary>
    public bool HasImage => Contents?.OfType<ImageContent>().Any() == true;

    /// <summary>
    /// Helper: Check if response contains audio
    /// </summary>
    public bool HasAudio => Contents?.OfType<AudioContent>().Any() == true;

    /// <summary>
    /// Helper: Get first image content
    /// </summary>
    public ImageContent? GetImage() => Contents?.OfType<ImageContent>().FirstOrDefault();

    /// <summary>
    /// Helper: Get first audio content
    /// </summary>
    public AudioContent? GetAudio() => Contents?.OfType<AudioContent>().FirstOrDefault();
}
```

### 5. Modifiche `SceneManager.cs`

#### a. `InitializeContextAsync`

```csharp
private Task<SceneContext> InitializeContextAsync(
    MultiModalInput input,
    Dictionary<string, object>? metadata,
    SceneRequestSettings settings,
    CancellationToken cancellationToken)
{
    var context = new SceneContext
    {
        ServiceProvider = _serviceProvider,
        Input = input,
        Metadata = metadata ?? new Dictionary<string, object>(),
        ChatClientManager = _chatClientManager,
        ConversationKey = settings.ConversationKey ?? Guid.NewGuid().ToString(),
        CacheBehavior = settings.CacheBehavior
    };

    return Task.FromResult(context);
}
```

#### b. `RequestAsync` - Scene Selection

```csharp
private async IAsyncEnumerable<AiSceneResponse> RequestAsync(
    SceneContext context,
    SceneRequestSettings settings,
    [System.Runtime.CompilerServices.EnumeratorCancellation] CancellationToken cancellationToken)
{
    // Create user message with multi-modal content
    var userMessage = context.Input.ToChatMessage();

    // Get all scenes as tools for selection
    var sceneTools = _sceneFactory.GetSceneNames()
        .Select(name => _sceneFactory.Create(name))
        .Select(scene => CreateSceneSelectionTool(scene))
        .ToList();

    var chatOptions = new ChatOptions
    {
        Tools = sceneTools.Cast<AITool>().ToList()
    };

    // Call LLM for scene selection (now with multi-modal input)
    var responseWithCost = await context.ChatClientManager.GetResponseAsync(
        new[] { userMessage },
        chatOptions,
        cancellationToken);

    // ... rest of the method
}
```

#### c. `ExecuteSceneAsync` - Tool Execution

```csharp
private async IAsyncEnumerable<AiSceneResponse> ExecuteSceneAsync(
    SceneContext context,
    IScene scene,
    SceneRequestSettings settings,
    [System.Runtime.CompilerServices.EnumeratorCancellation] CancellationToken cancellationToken)
{
    // ... existing code for MCP and scene tools

    // Build conversation messages with multi-modal support
    var conversationMessages = new List<ChatMessage>
    {
        context.Input.ToChatMessage() // Now includes text + images/audio/files
    };

    // Add main actor context
    if (context.MainActorContext.Count > 0)
    {
        var mainActorText = string.Join("\n\n", context.MainActorContext);
        conversationMessages.Add(new ChatMessage(ChatRole.System, mainActorText));
    }

    // ... rest of the method (unchanged - tool execution loop works the same)
}
```

#### d. `ProcessStreamingChunkAsync` - Multi-Modal Response

```csharp
private async IAsyncEnumerable<AiSceneResponse> ProcessStreamingChunkAsync(
    ChatResponseUpdate streamChunk,
    string? sceneName,
    SceneContext context,
    SceneRequestSettings settings,
    [System.Runtime.CompilerServices.EnumeratorCancellation] CancellationToken cancellationToken)
{
    // Accumulate complete message
    var contextKey = $"streaming_message_{sceneName ?? "final"}";
    if (!context.Properties.TryGetValue(contextKey, out var accumulatedObj))
    {
        accumulatedObj = new System.Text.StringBuilder();
        context.Properties[contextKey] = accumulatedObj;
    }
    var accumulated = (System.Text.StringBuilder)accumulatedObj;

    // Get the text from this chunk
    var chunkText = streamChunk.Text ?? string.Empty;
    accumulated.Append(chunkText);

    // Check if this is the final chunk
    var isComplete = streamChunk.FinishReason != null;

    var streamResponse = new AiSceneResponse
    {
        Status = isComplete ? AiResponseStatus.Running : AiResponseStatus.Streaming,
        SceneName = sceneName,
        StreamingChunk = chunkText,
        Message = accumulated.ToString(),
        IsStreamingComplete = isComplete
    };

    // NEW: Handle multi-modal response contents (images/audio generated)
    if (isComplete && streamChunk.Contents != null)
    {
        var multiModalContents = streamChunk.Contents
            .Where(c => c is ImageContent or AudioContent or DataContent)
            .ToList();

        if (multiModalContents.Any())
        {
            streamResponse.Contents = multiModalContents;
            _logger.LogInformation(
                "Multi-modal response received: {ImageCount} images, {AudioCount} audio (Scene: {SceneName})",
                multiModalContents.OfType<ImageContent>().Count(),
                multiModalContents.OfType<AudioContent>().Count(),
                sceneName);
        }
    }

    // ... rest of the method
}
```

## üìù Usage Examples

### 1. **Analisi Immagine**

```csharp
var input = MultiModalInput.FromImageUrl(
    text: "What's in this image?",
    imageUrl: "https://example.com/image.jpg"
);

await foreach (var response in sceneManager.ExecuteAsync(input))
{
    if (response.Status == AiResponseStatus.Running)
    {
        Console.WriteLine($"AI: {response.Message}");
    }
}
```

### 2. **Analisi Audio**

```csharp
var audioBytes = File.ReadAllBytes("recording.mp3");
var input = new MultiModalInput
{
    Text = "Transcribe this audio",
    Contents = 
    [
        new AudioContent(Convert.ToBase64String(audioBytes), "audio/mp3")
    ]
};

await foreach (var response in sceneManager.ExecuteAsync(input))
{
    if (response.Status == AiResponseStatus.Running)
    {
        Console.WriteLine($"Transcription: {response.Message}");
    }
}
```

### 3. **Analisi Documento PDF**

```csharp
var input = MultiModalInput.FromFileUrl(
    text: "Summarize this document",
    fileUrl: "https://example.com/report.pdf"
);

await foreach (var response in sceneManager.ExecuteAsync(input))
{
    if (response.Status == AiResponseStatus.Running)
    {
        Console.WriteLine($"Summary: {response.Message}");
    }
}
```

### 4. **Multi-Modal Combo (Immagine + Audio + Testo)**

```csharp
var input = new MultiModalInput
{
    Text = "Compare this image with the audio description",
    Contents =
    [
        new ImageContent("https://example.com/product.jpg"),
        new AudioContent("https://example.com/review.mp3")
    ]
};

await foreach (var response in sceneManager.ExecuteAsync(input, settings: new()
{
    EnableStreaming = true
}))
{
    if (response.Status == AiResponseStatus.Streaming)
    {
        Console.Write(response.StreamingChunk); // Progressive output
    }
    else if (response.IsStreamingComplete)
    {
        Console.WriteLine($"\n\nComplete: {response.Message}");
        
        // Check for generated image
        if (response.HasImage)
        {
            var image = response.GetImage();
            Console.WriteLine($"Generated image: {image?.Uri}");
        }
    }
}
```

### 5. **Backward Compatibility (Text-Only)**

```csharp
// Still works exactly as before!
await foreach (var response in sceneManager.ExecuteAsync("Hello, world!"))
{
    Console.WriteLine(response.Message);
}
```

## üîß Implementation Steps

### Phase 1: Foundation (Core Models)
1. ‚úÖ Create `MultiModalInput` class
2. ‚úÖ Modify `ISceneManager` with overload
3. ‚úÖ Modify `SceneContext` to use `MultiModalInput`
4. ‚úÖ Modify `AiSceneResponse` to support multi-modal output
5. ‚úÖ Update `SceneRequestSettings` (optional: add multi-modal specific settings)

### Phase 2: Scene Manager Integration
1. ‚úÖ Update `InitializeContextAsync` to use `MultiModalInput`
2. ‚úÖ Update `RequestAsync` (scene selection) to handle multi-modal input
3. ‚úÖ Update `ExecuteSceneAsync` (tool execution) to handle multi-modal input
4. ‚úÖ Update `ProcessStreamingChunkAsync` to handle multi-modal response
5. ‚úÖ Update `GenerateFinalResponseAsync` to support multi-modal

### Phase 3: Tool & Actor Integration
1. ‚úÖ Update `IActor` to optionally access multi-modal inputs
2. ‚úÖ Update `ISceneTool` to optionally return multi-modal outputs
3. ‚úÖ Create helper tools for common multi-modal operations:
   - `ImageAnalysisTool`
   - `AudioTranscriptionTool`
   - `DocumentParsingTool`

### Phase 4: Testing & Documentation
1. ‚úÖ Unit tests for multi-modal input parsing
2. ‚úÖ Integration tests with GPT-4 Vision, Whisper, etc.
3. ‚úÖ Update STREAMING.md with multi-modal examples
4. ‚úÖ Create MULTI_MODAL.md comprehensive guide
5. ‚úÖ Add examples to documentation

## üé® Advanced Features (Future)

### 1. **Multi-Modal Caching**
Cache image/audio analysis results to avoid redundant API calls

```csharp
public sealed class SceneRequestSettings
{
    /// <summary>
    /// Cache multi-modal content analysis (images, audio) 
    /// </summary>
    public bool CacheMultiModalAnalysis { get; set; } = true;

    /// <summary>
    /// Cache expiration for multi-modal content (default: 1 hour)
    /// </summary>
    public TimeSpan MultiModalCacheExpiration { get; set; } = TimeSpan.FromHours(1);
}
```

### 2. **Multi-Modal Memory**
Store visual/audio context in conversation memory

```csharp
public sealed class ConversationMemory
{
    /// <summary>
    /// Previously seen images in this conversation
    /// </summary>
    public List<ImageReference> PreviousImages { get; set; } = [];

    /// <summary>
    /// Previously processed audio in this conversation
    /// </summary>
    public List<AudioReference> PreviousAudio { get; set; } = [];
}
```

### 3. **Multi-Modal Tool Calling**
Tools can request specific multi-modal inputs

```csharp
services.AddPlayFramework(play => { })
    .AddScene("ImageAnalysis", "Analyzes images")
    .WithTool<IImageProcessingService>(tool =>
    {
        tool.WithMethod(x => x.DetectObjects(default!), 
            name: "DetectObjects",
            description: "Detects objects in an image")
            .RequiresImage(); // NEW: Specifies tool needs image input
    });
```

## üèÜ Benefits

1. **Backward Compatible**: Existing text-only code continues to work
2. **Type-Safe**: Strongly typed multi-modal content with AIContent hierarchy
3. **Streaming-Compatible**: Multi-modal inputs work seamlessly with streaming responses
4. **Extensible**: Easy to add new content types (video, 3D models, etc.)
5. **Tool-Integrated**: Tools can consume and produce multi-modal content
6. **Memory-Aware**: Conversation memory can track multi-modal interactions

## üìö Microsoft.Extensions.AI Support

Il framework usa gi√† `Microsoft.Extensions.AI` che supporta nativamente:

```csharp
// ‚úÖ ImageContent - gi√† supportato
new ImageContent(imageUri, mimeType);
new ImageContent(imageBytes, mimeType);

// ‚úÖ AudioContent - gi√† supportato
new AudioContent(audioUri, mimeType);
new AudioContent(audioBytes, mimeType);

// ‚úÖ DataContent - per file/documenti
new DataContent(fileUri, mimeType);
new DataContent(fileBytes, mimeType);

// ‚úÖ TextContent - gi√† usato
new TextContent(text);
```

## ‚ö†Ô∏è Considerations

1. **Provider Support**: Non tutti i provider LLM supportano multi-modalit√†:
   - ‚úÖ GPT-4 Vision (images)
   - ‚úÖ Claude 3.5+ (images)
   - ‚úÖ Gemini Pro Vision (images, audio)
   - ‚úÖ Whisper (audio ‚Üí text)
   - ‚ùå Standard GPT-3.5, LLaMA (text-only)

2. **Cost Impact**: Multi-modal inputs aumentano significativamente i costi:
   - Images: ~$0.01-0.03 per image
   - Audio: ~$0.006 per minute
   - Files: Variabile in base al token count

3. **Rate Limits**: Multi-modal requests hanno rate limits pi√π stringenti

4. **Size Limits**: 
   - Images: Max 20MB, 2000x2000px recommended
   - Audio: Max 25MB
   - Files: Variabile per provider

## üöÄ Ready to Implement?

Posso iniziare l'implementazione seguendo questo piano. Vuoi che:

1. Implemento Phase 1 (Foundation) con i modelli base?
2. Implemento tutto insieme con test completi?
3. Creo prima esempi/prototipi per validare l'approccio?

---

**Prossimi Steps**:
- [ ] Review del piano
- [ ] Implementazione Phase 1
- [ ] Testing con GPT-4 Vision
- [ ] Documentazione completa
